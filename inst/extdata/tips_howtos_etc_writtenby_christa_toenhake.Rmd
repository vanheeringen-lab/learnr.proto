---
title: "My evaluation with notes, tips, advice, additional ideas etc.  "
author: "Christa Toenhake"
date: "26/01/2020"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE, echo = T, eval = F}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
In this rmd I wrote down  

- Things I/we learned along the way (and could be applied more in the beginning).  
- Ideas for other questions (we had or I had but left out because of time-constrains).  
- Anything else that might be worth to tell the next person working on this.  

# Things I/we learned along the way  

### chunk headers  
Problem: Hints did not always display properly.  
Solution: Turned out TRUE should be written as 'TRUE' in chunk headings when working with learnr. If you use for example `exercise=T`, the hints will not be displayed (annoyingly).  

### coding exercises  
Problem: For fill-in-the-code exercises, we add '___' to the exercise chunk but this gives an error when 'Run Document' is done.   
Solution: You should add 'error=TRUE, exercise.eval=FALSE' to the exercise chunk headings.   

### giving feedback  
Checking answers is done with `gradethis` package. There are (at least, as far as I used) two options for answer checking:  

1. `grade_result()`   

  - use the chunk title `[exercise_chunk_name]-check` when you use `grade_result()`.   
  - formulate as, for example:  

```{r graderesult_example, echo=TRUE, eval=FALSE}
grade_result(pass_if(~identical(.result, head(distance2NearestFeature(monocytes_h3k4me3, tss_chr19)))))
```

 - You can use additional argument `correct = [text]` to customize the message displayed when the submitted answer is correct.   
 - In **fg3** I realized that we can also compare to plots, eg:  
 
```{r q8_heatmap1mark-check, echo=TRUE, eval=FALSE}
grade_result(
 pass_if(~identical(.result, heatMatrix(mat = scores_h3k4me3_tss, 
           xcoord = c(-5000, 5000), 
           col = h3k4me3_cols, 
           main = "Unordered H3K4me3 signal, centered at TSS", 
           xlab = "bp (TSS = 0)")
           )),
 correct = 
)
```
  

2. `grade_code( [compare the answer with this answer])`   

  - use the chunk title `[exercise_chunk_name]-code-check` when you use `grade_code()`. 
  - provide the code solution in the chunk with title `[exercise_chunk_name]-solution`
  - this option works towards the exact code given in the solution chunk with nice feedback. **BUT** for the answer to be correct the code should really be exactly the same as the solution. Eg: head(a, 2) =/= head(a, n=2).   
  - another **BUT**; when you want students to generate the plot and you use this method of grading, the plot won't be visualized. Work-a-round1: direct students to hit "Run Code" explicitely before or after "Submit Answer". Work-a-round2: use grade_result() instead.  
  - another **BUT** not handy if you multiple lines of code and leave blanks. In that case, students might fill out one line and hit "Submit" but will receive weird, not-understandable feedback due to the blanks in the other lines of not-yet-corrected code. Work-a-round1: direct students to fill in *all* blanks before hitting "Submit Answer". Work-a-round2: use grade_result() instead.  
  - preferentially leave out hints for this type of answer checking.  
  - however, I still added hints to grade_code questions when there were multiple functions where they had to complete the code. This is not really optimal use of learnr. An alternative would have been to chop these questions up in smaller parts but then I would have to prepare setup chunks for each invidual, chopped-up-exercise (as I understood it).


``{r naam-solution}
exact de code die je verwacht
``

``{r naam-code-check}
grade_code()
``


* I realized later on that also plots can be given 
